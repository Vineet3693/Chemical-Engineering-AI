"""
LLM Handler Module
Manages Google Gemini API integration for response generation
"""

import google.generativeai as genai
from typing import List, Dict, Any, Generator
from config.settings import settings
from utils.logger import get_logger, log_execution_time

# Initialize logger
logger = get_logger(__name__)


class LLMHandler:
    """Handle Google Gemini LLM interactions"""
    
    def __init__(self, api_key: str = None):
        """
        Initialize Gemini LLM
        
        Args:
            api_key: Google API key (default from settings)
        """
        logger.info("Initializing LLM Handler...")
        self.api_key = api_key or settings.GOOGLE_API_KEY
        
        if not self.api_key:
            logger.error("Google API key is missing")
            raise ValueError("Google API key is required")
        
        try:
            # Configure Gemini
            genai.configure(api_key=self.api_key)
            
            # Initialize model
            self.model = genai.GenerativeModel(
                model_name=settings.LLM_MODEL,
                generation_config={
                    'temperature': settings.LLM_TEMPERATURE,
                    'top_p': settings.LLM_TOP_P,
                    'top_k': settings.LLM_TOP_K,
                    'max_output_tokens': settings.LLM_MAX_TOKENS,
                }
            )
            
            logger.info(f"Gemini LLM initialized | Model: {settings.LLM_MODEL} | Temp: {settings.LLM_TEMPERATURE}")
        except Exception as e:
            logger.error(f"Failed to initialize LLM: {type(e).__name__}: {str(e)}")
            raise
    
    @log_execution_time
    def generate_response(self, prompt: str) -> str:
        """
        Generate a response from Gemini
        
        Args:
            prompt: Input prompt
            
        Returns:
            Generated response text
        """
        prompt_length = len(prompt)
        logger.debug(f"Generating response | Prompt length: {prompt_length} chars")
        
        try:
            response = self.model.generate_content(prompt)
            try:
                response_text = response.text
                logger.info(f"Response generated | Length: {len(response_text)} chars")
                return response_text
            except ValueError:
                # Handle multi-part or other complex responses
                if response.candidates:
                    response_text = "".join([part.text for part in response.candidates[0].content.parts])
                    logger.info(f"Response generated (multi-part) | Length: {len(response_text)} chars")
                    return response_text
                logger.warning("No content generated by LLM")
                return "Error: No content generated"
        except Exception as e:
            logger.error(f"Error generating response: {type(e).__name__}: {str(e)}")
            return f"Error: Unable to generate response. {str(e)}"
    
    def stream_response(self, prompt: str) -> Generator[str, None, None]:
        """
        Stream response from Gemini (for better UX)
        
        Args:
            prompt: Input prompt
            
        Yields:
            Response text chunks
        """
        logger.debug(f"Streaming response | Prompt length: {len(prompt)} chars")
        chunk_count = 0
        
        try:
            response = self.model.generate_content(prompt, stream=True)
            for chunk in response:
                if chunk.text:
                    chunk_count += 1
                    yield chunk.text
            logger.info(f"Streaming complete | Chunks: {chunk_count}")
        except Exception as e:
            logger.error(f"Streaming error: {type(e).__name__}: {str(e)}")
            yield f"Error: {str(e)}"
    
    def create_rag_prompt(self, query: str, context_chunks: List[Dict[str, Any]]) -> str:
        """
        Create a prompt for RAG with context from books
        
        Args:
            query: User question
            context_chunks: Retrieved chunks with metadata
            
        Returns:
            Formatted prompt
        """
        logger.debug(f"Creating RAG prompt | Query: '{query[:50]}...' | Context chunks: {len(context_chunks)}")
        
        # Format context with sources
        context_text = ""
        for i, chunk in enumerate(context_chunks, 1):
            book = chunk['metadata'].get('book_name', 'Unknown')
            page = chunk['metadata'].get('page', 'N/A')
            text = chunk['text']
            context_text += f"\n[Source {i}: {book}, Page {page}]\n{text}\n"
        
        prompt = f"""You are an expert Chemical Engineering assistant. Answer the following question based ONLY on the provided context from Chemical Engineering textbooks.

Context from textbooks:
{context_text}

Question: {query}

Instructions:
1. Provide a detailed, technical answer based on the context above
2. Use specific information from the sources
3. If the context doesn't contain enough information to fully answer the question, say so
4. Be precise with technical terminology and concepts
5. Reference the sources naturally in your answer (e.g., "According to [Source 1]...")

Answer:"""
        
        logger.info(f"RAG prompt created | Total length: {len(prompt)} chars")
        return prompt
    
    def create_general_knowledge_prompt(self, query: str) -> str:
        """
        Create a prompt for general knowledge mode (internet search alternative)
        
        Args:
            query: User question
            
        Returns:
            Formatted prompt
        """
        logger.debug(f"Creating general knowledge prompt | Query: '{query[:50]}...'")
        
        prompt = f"""You are an expert Chemical Engineering assistant with access to general knowledge and current information.

Question: {query}

Instructions:
1. Provide a comprehensive, technical answer using your general knowledge
2. Include current best practices and recent developments if relevant
3. Be precise with technical terminology and concepts
4. Explain complex concepts clearly
5. Note: This answer is based on general knowledge, not specific textbook references

Answer:"""
        
        logger.info(f"General knowledge prompt created | Length: {len(prompt)} chars")
        return prompt
    
    def format_technical_prompt(self, question: str, context: str = None) -> str:
        """
        Format a prompt optimized for technical/scientific content
        
        Args:
            question: User question
            context: Optional context
            
        Returns:
            Formatted prompt
        """
        if context:
            return f"""Context: {context}

Question: {question}

Provide a detailed technical answer based on the context above."""
        else:
            return f"""Question: {question}

Provide a detailed technical answer using your knowledge of Chemical Engineering."""


# Example usage
if __name__ == "__main__":
    try:
        llm = LLMHandler()
        
        # Test general knowledge
        prompt = llm.create_general_knowledge_prompt("What is distillation?")
        response = llm.generate_response(prompt)
        print(f"Response: {response}")
        
    except Exception as e:
        print(f"Error: {e}")
